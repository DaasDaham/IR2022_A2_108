{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCXpPp5B9Eiv"
      },
      "outputs": [],
      "source": [
        "!unzip Humor,Hist,Media,Food.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq2fWRg0Abv9",
        "outputId": "b00150c3-8eaa-42b9-80fa-5971e62af61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import time\n",
        "import string\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score, cohen_kappa_score\n",
        "import heapq\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from math import exp, sqrt, pi\n",
        "from scipy import spatial\n",
        "import copy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "E0rn8CT2AbwG"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
        "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "stopwords = nltk.corpus.stopwords.words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhNuOAqBAbwI"
      },
      "source": [
        "# Q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU_7s1arBMQK"
      },
      "source": [
        "## a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WV59pM6YAbwL"
      },
      "outputs": [],
      "source": [
        "def preprocess(line):\n",
        "  '''\n",
        "  Preprocess the data\n",
        "    - Lowercase\n",
        "    - Remove Extra Spaces\n",
        "    - Lemmatize using WordNetLemmatizer()\n",
        "    - Tokenize using RegexpTokenizer()\n",
        "    - Remove stopwords\n",
        "  '''\n",
        "  line = \" \".join(line.lower().split()) #lowercase and remove extra spaces\n",
        "  #line = line.apply(lambda x: nltk.word_tokenize(x)) #tokenize\n",
        "  line = [word for word in line.split() if not word in stopwords] #remove stopwords\n",
        "  line = [lemmatizer.lemmatize(word) for word in line] #lemmatize\n",
        "  line = tokenizer.tokenize(\" \".join(line)) #tokenize\n",
        "  return set(line)\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "for n, filee in enumerate(glob.glob('Humor,Hist,Media,Food/*')):\n",
        "  with open(filee, \"r\", encoding='windows-1254', errors='ignore') as fp:\n",
        "    fp = fp.read()\n",
        "    word_set = preprocess(fp)\n",
        "    data_dict[filee.split(\"/\")[-1]] = word_set\n",
        "\n",
        "#save the index\n",
        "with open('word_index.pkl', 'wb') as fp:\n",
        "    pickle.dump(data_dict, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JhS0ylhGAbwO"
      },
      "outputs": [],
      "source": [
        "with open('word_index.pkl', 'rb') as fp:\n",
        "    data_dict = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9kncHag0AbwP"
      },
      "outputs": [],
      "source": [
        "def preprocessQuery(line):\n",
        "  '''\n",
        "  Preprocess the query\n",
        "  '''\n",
        "  line = \" \".join(line.lower().split()) #lowercase and remove extra spaces\n",
        "  line = [word for word in line.split() if not word in nltk.corpus.stopwords.words()] #remove stopwords\n",
        "  line = [lemmatizer.lemmatize(word) for word in line] #lemmatize\n",
        "  line = tokenizer.tokenize(\" \".join(line)) #tokenize\n",
        "  return set(line)\n",
        "\n",
        "def calculate_intersect(query, document, document_name):\n",
        "  '''\n",
        "  calculate intersection between doc and query\n",
        "  '''\n",
        "  return (len(query.intersection(document))/len(query.union(document)), document_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mh9H4AwAbwR",
        "outputId": "e7fc5e87-6b13-47ec-c745-82f223b18c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'thoughtfully', 'lion', 'moment', 'stood'}\n"
          ]
        }
      ],
      "source": [
        "query = 'lion stood thoughtfully for a moment'\n",
        "query = preprocessQuery(query)\n",
        "\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "iI5z4aVhAbwV"
      },
      "outputs": [],
      "source": [
        "output = []\n",
        "for i in data_dict:\n",
        "    output.append(calculate_intersect(query, data_dict[i], i))\n",
        "\n",
        "heapq.heapify(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXLj0gxDAbwX",
        "outputId": "e7cc101e-fdbf-4172-da6d-5f64ae21db5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.012048192771084338, 'vonthomp'),\n",
              " (0.01015228426395939, 'solders.hum'),\n",
              " (0.00909090909090909, 'puzzles.jok'),\n",
              " (0.006802721088435374, 'wedding.hum'),\n",
              " (0.006688963210702341, 'phorse.hum')]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "heapq.nlargest(5, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz4Mk0jXAbwY"
      },
      "source": [
        "## b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "1XrS7IXtAbwZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(line):\n",
        "  '''\n",
        "  Preprocess the data\n",
        "    - Lowercase\n",
        "    - Remove Extra Spaces\n",
        "    - Lemmatize using WordNetLemmatizer()\n",
        "    - Tokenize using RegexpTokenizer()\n",
        "    - Remove stopwords\n",
        "  '''\n",
        "  line = \" \".join(line.lower().split()) #lowercase and remove extra spaces\n",
        "  #line = line.apply(lambda x: nltk.word_tokenize(x)) #tokenize\n",
        "  line = [word for word in line.split() if not word in stopwords] #remove stopwords\n",
        "  line = [lemmatizer.lemmatize(word) for word in line] #lemmatize\n",
        "  line = tokenizer.tokenize(\" \".join(line)) #tokenize\n",
        "  return nltk.FreqDist(line)\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "for n, filee in enumerate(glob.glob('Humor,Hist,Media,Food/*')):\n",
        "  with open(filee, \"r\", encoding='windows-1254', errors='ignore') as fp:\n",
        "    fp = fp.read()\n",
        "    freq_dict = preprocess(fp)\n",
        "  for i in freq_dict:\n",
        "    if i not in data_dict:\n",
        "      data_dict[i] = [((n, filee.split(\"/\")[-1]), freq_dict[i])]\n",
        "    else:\n",
        "      data_dict[i].append(((n, filee.split(\"/\")[-1]), freq_dict[i]))\n",
        "\n",
        "#save the index\n",
        "with open('frequency_index.pkl', 'wb') as fp:\n",
        "    pickle.dump(data_dict, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "U_fejUePAbwa"
      },
      "outputs": [],
      "source": [
        "with open('frequency_index.pkl', 'rb') as fp:\n",
        "    data_dict = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "z6dHIjwfAbwb"
      },
      "outputs": [],
      "source": [
        "filenames = glob.glob('Humor,Hist,Media,Food/*')\n",
        "vocab = list(data_dict.keys())\n",
        "tf_idf_mat = np.zeros((len(filenames), len(data_dict)))\n",
        "\n",
        "def preprocessQuery(line):\n",
        "  '''\n",
        "  Preprocess the query\n",
        "  '''\n",
        "  line = \" \".join(line.lower().split()) #lowercase and remove extra spaces\n",
        "  line = [word for word in line.split() if not word in nltk.corpus.stopwords.words()] #remove stopwords\n",
        "  line = [lemmatizer.lemmatize(word) for word in line] #lemmatize\n",
        "  line = tokenizer.tokenize(\" \".join(line)) #tokenize\n",
        "  return set(line)\n",
        "\n",
        "for n, i in enumerate(list(data_dict.keys())):\n",
        "    for doc in data_dict[i]:\n",
        "        tf_idf_mat[doc[0][0]][n] = doc[1]\n",
        "\n",
        "def binary(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        score = 0\n",
        "        for i in query:\n",
        "            score += 1 * np.log(len(filenames)/(matrix[doc][i] + 1)) if matrix[doc][i] > 0 else 0\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "def binary_cos(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames), len(data_dict)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        score = 0\n",
        "        query_vec = []\n",
        "        for i in matrix[doc]:\n",
        "            query_vec.append(1 * np.log(len(filenames)/(i + 1)) if i > 0 else 0)\n",
        "        score = 1 - spatial.distance.cosine(query_vec, query)\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "\n",
        "def raw_count(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        score = 0\n",
        "        for i in query:\n",
        "            score += matrix[doc][i] * np.log(len(filenames)/(matrix[doc][i] + 1))\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "def raw_count_cos(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        target_list = []\n",
        "        for i in matrix[doc]:\n",
        "            target_list.append(i * np.log(len(filenames)/(i + 1)))\n",
        "        score = 1 - spatial.distance.cosine(query, target_list)\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "def term_freq(query, matrix):\n",
        "    score_store = np.zeros((len(filenames), len(query)))\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        for n, i in enumerate(query):\n",
        "            score_store[doc][n] = matrix[doc][i]\n",
        "    sum_scores_per_doc = np.sum(matrix, axis = 1)\n",
        "    score_store_tf = score_store/np.array([sum_scores_per_doc for i in range(len(query))]).T\n",
        "    idf = np.log(len(filenames)/(score_store + 1))\n",
        "    score_store_tf_idf = score_store_tf * idf\n",
        "    sum_scores = np.sum(score_store_tf_idf, axis = 1)\n",
        "    sum_scores = [(x, filenames[n]) for n, x in enumerate(sum_scores)]\n",
        "    heapq.heapify(sum_scores)\n",
        "    return heapq.nlargest(5, sum_scores)\n",
        "\n",
        "def term_freq_cos(query, matrix):\n",
        "    score_store = copy.deepcopy(matrix)\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    sum_scores_per_doc = np.sum(matrix, axis = 1)\n",
        "    score_store_tf = score_store/np.array([sum_scores_per_doc for i in range(len(query))]).T\n",
        "    idf = np.log(len(filenames)/(score_store + 1))\n",
        "    score_store_tf_idf = score_store_tf * idf\n",
        "    similarity_scores = score_store_tf_idf.dot(query)/ (np.linalg.norm(score_store_tf_idf, axis=1) * np.linalg.norm(query))\n",
        "    #sum_scores = np.sum(score_store_tf_idf, axis = 1)\n",
        "    sum_scores = [(x, filenames[n]) for n, x in enumerate(similarity_scores)]\n",
        "    heapq.heapify(sum_scores)\n",
        "    return heapq.nlargest(5, sum_scores)\n",
        "\n",
        "def log_norm(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        score = 0\n",
        "        for i in query:\n",
        "            score += np.log(1 + matrix[doc][i]) * np.log(len(filenames)/(matrix[doc][i] + 1))\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "def log_norm_cos(query, matrix):\n",
        "    score_list = []\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        score = 0\n",
        "        query_vec = []\n",
        "        for i in matrix[doc]:\n",
        "            query_vec.append(np.log(1 + i) * np.log(len(filenames)/(i + 1)))\n",
        "        score = 1 - spatial.distance.cosine(query_vec, query)\n",
        "        score_list.append((score, filenames[doc]))\n",
        "    heapq.heapify(score_list)\n",
        "    return heapq.nlargest(5, score_list)\n",
        "\n",
        "def double_norm(query, matrix):\n",
        "    score_store = np.zeros((len(filenames), len(query)))\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    for doc in range(len(doc_list)):\n",
        "        for n, i in enumerate(query):\n",
        "            score_store[doc][n] = matrix[doc][i]\n",
        "    max_query = matrix.max(axis = 1)\n",
        "    score_store_tf = 0.5 + 0.5*(score_store/np.array([max_query for i in range(len(query))]).T)\n",
        "    idf = np.log(len(filenames)/(score_store + 1))\n",
        "    score_store_tf_idf = score_store_tf * idf\n",
        "    sum_scores = np.sum(score_store_tf_idf, axis = 1)\n",
        "    sum_scores = [(x, filenames[n]) for n, x in enumerate(sum_scores)]\n",
        "    heapq.heapify(sum_scores)\n",
        "    return heapq.nlargest(5, sum_scores)\n",
        "\n",
        "def double_norm_cos(query, matrix):\n",
        "    score_store = copy.deepcopy(matrix)\n",
        "    doc_list = np.zeros((len(filenames)))\n",
        "    max_query = matrix.max(axis = 1)\n",
        "    score_store_tf = 0.5 + 0.5*(score_store/np.array([max_query for i in range(len(query))]).T)\n",
        "    idf = np.log(len(filenames)/(score_store + 1))\n",
        "    score_store_tf_idf = score_store_tf * idf\n",
        "    similarity_scores = score_store_tf_idf.dot(query)/ (np.linalg.norm(score_store_tf_idf, axis=1) * np.linalg.norm(query))\n",
        "    #sum_scores = np.sum(score_store_tf_idf, axis = 1)\n",
        "    sum_scores = [(x, filenames[n]) for n, x in enumerate(similarity_scores)]\n",
        "    heapq.heapify(sum_scores)\n",
        "    return heapq.nlargest(5, sum_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3cUY-EsAbwe",
        "outputId": "a8d130ea-4b98-40c5-e1c2-ebe6555332b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'lion'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = 'lion'\n",
        "query = preprocessQuery(query)\n",
        "print(query)\n",
        "\n",
        "q = np.zeros(((len(vocab))))\n",
        "for word in query:\n",
        "    q[vocab.index(word)] = 1\n",
        "q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwAfz25S8Bpn"
      },
      "source": [
        "### Binary\n",
        " - Pros: Basic Method\n",
        " - Cons: Does not take into account the number of occurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIP5fyw5Abwf",
        "outputId": "75f29fdf-e16b-4f7a-a35c-676043a61cf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(6.339477080468061, 'Humor,Hist,Media,Food/wagon.hum'),\n",
              " (6.339477080468061, 'Humor,Hist,Media,Food/tpquotes.txt'),\n",
              " (6.339477080468061, 'Humor,Hist,Media,Food/tnd.1'),\n",
              " (6.339477080468061, 'Humor,Hist,Media,Food/three.txt'),\n",
              " (6.339477080468061, 'Humor,Hist,Media,Food/puzzles.jok')]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "binary(np.where(q == 1)[0], tf_idf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEIxoskMDFdH",
        "outputId": "9d86dc48-bec9-473d-b29d-51cf4ea7e275"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.09797382495567952, 'Humor,Hist,Media,Food/puzzles.jok'),\n",
              " (0.06600114920766997, 'Humor,Hist,Media,Food/hecomes.jok'),\n",
              " (0.06509361397080737, 'Humor,Hist,Media,Food/booze1.fun'),\n",
              " (0.053009783807091204, 'Humor,Hist,Media,Food/three.txt'),\n",
              " (0.050484563345917, 'Humor,Hist,Media,Food/wagon.hum')]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "binary_cos(q, tf_idf_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLeLAhuH8Bpp"
      },
      "source": [
        "### Raw Count\n",
        " - Pros: Takes into account the number of occurences\n",
        " - Cons: Large Documents are bound to have larger no. of occurences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9S576BOAbwf",
        "outputId": "39279e35-8f82-4ea5-8c0d-78634fb347e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(171.39503539309263, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (161.3129313665778, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (157.15040766477554, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (57.11637010836572, 'Humor,Hist,Media,Food/boneles2.txt'),\n",
              " (16.938989699724345, 'Humor,Hist,Media,Food/stuf10.txt')]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_count(np.where(q == 1)[0], tf_idf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf4M4UWDDJ8y",
        "outputId": "f6b1e322-073a-4e1b-a614-324a66536b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.43358886578151656, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (0.43219096261072565, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (0.3826577318173554, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (0.1484552943208155, 'Humor,Hist,Media,Food/boneles2.txt'),\n",
              " (0.07344478544514443, 'Humor,Hist,Media,Food/puzzles.jok')]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_count_cos(q, tf_idf_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyStw6yI8Bpq"
      },
      "source": [
        "### Term Frequency\n",
        " - Pros: Takes into account the number of occurences and normalizes them based on the size of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4A8cj8pAbwh",
        "outputId": "7041615a-bb4a-4e4c-fcf4-f27e87489ecb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.1613453877461761, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (0.15175252245209578, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (0.12649080102811266, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (0.04766524120652677, 'Humor,Hist,Media,Food/puzzles.jok'),\n",
              " (0.03939060007473498, 'Humor,Hist,Media,Food/boneles2.txt')]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "term_freq(np.where(q == 1)[0], tf_idf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHtFqxAzDN4A",
        "outputId": "8e932cd1-10b0-4f61-9856-b030a0ae1553"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.43358886578151656, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (0.4321909626107256, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (0.3826577318173556, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (0.1484552943208155, 'Humor,Hist,Media,Food/boneles2.txt'),\n",
              " (0.07344478544514443, 'Humor,Hist,Media,Food/puzzles.jok')]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "term_freq_cos(q, tf_idf_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0i0jK0h8Bpr"
      },
      "source": [
        "### Log Normalization\n",
        " - Pros: Log normailized Basic Method\n",
        " - Cons: Does not take into account the number of occurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4_zae0VAbwh",
        "outputId": "57b1631f-045a-48ab-bb38-5b6de8fbd219"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(12.175285512901077, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (12.141032258977294, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (12.049477146808298, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (11.594875013500031, 'Humor,Hist,Media,Food/boneles2.txt'),\n",
              " (7.827475301265257, 'Humor,Hist,Media,Food/stuf10.txt')]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_norm(np.where(q == 1)[0], tf_idf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3WHKNWGDRqz",
        "outputId": "3ac690ae-43e9-4010-d494-58170adb94d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.09545750141976062, 'Humor,Hist,Media,Food/lion.txt'),\n",
              " (0.09069150015369865, 'Humor,Hist,Media,Food/lion.jok'),\n",
              " (0.08589998316319192, 'Humor,Hist,Media,Food/puzzles.jok'),\n",
              " (0.0808484963678402, 'Humor,Hist,Media,Food/lions.cat'),\n",
              " (0.0717474284448123, 'Humor,Hist,Media,Food/boneles2.txt')]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_norm_cos(q, tf_idf_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG0OfTEA8Bps"
      },
      "source": [
        "### Double Normalization\n",
        " - Pros: Normalizes based on maximum frequency in a document\n",
        " - Cons: CAn lead to different results because it only used the max frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mt6xm6TAbwi",
        "outputId": "5943ba68-67ee-4428-abd3-a4a34d7c2810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(3.8036862482808367, 'Humor,Hist,Media,Food/puzzles.jok'),\n",
              " (3.565955857763284, 'Humor,Hist,Media,Food/wagon.hum'),\n",
              " (3.565955857763284, 'Humor,Hist,Media,Food/drunk.txt'),\n",
              " (3.5163121305140033, 'Humor,Hist,Media,Food/zuccmush.sal'),\n",
              " (3.5163121305140033, 'Humor,Hist,Media,Food/zucantom.sal')]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "double_norm(np.where(q == 1)[0], tf_idf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqKYv5UrDVQR",
        "outputId": "27cd5d60-d5f2-4907-d9d8-d156281b7ca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.004089083501565676, 'Humor,Hist,Media,Food/puzzles.jok'),\n",
              " (0.0038333917608823517, 'Humor,Hist,Media,Food/wagon.hum'),\n",
              " (0.0038333448046986335, 'Humor,Hist,Media,Food/drunk.txt'),\n",
              " (0.0038320392397851954, 'Humor,Hist,Media,Food/mlverb.hum'),\n",
              " (0.0038293514311648065, 'Humor,Hist,Media,Food/humor9.txt')]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "double_norm_cos(q, tf_idf_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t4IjQIfHLTW"
      },
      "source": [
        "# Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbh-pfH4HN8N",
        "outputId": "09d8b616-9e8c-472a-915d-12a211159bd2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "docs = []\n",
        "with open(\"IR-assignment-2-data.txt\", \"r\") as data_file:\n",
        "    for line in data_file:\n",
        "        line = line.split(\" \")\n",
        "        relevance_score = float(line[0])\n",
        "        qid = int(line[1].split(\":\")[1])\n",
        "        if qid == 4:\n",
        "            docs.append([relevance_score, qid, line[74].split(\":\")[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jb222X4uHN-9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>qid</th>\n",
              "      <th>sum_tf_idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>30.667985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>42.200053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>30.667985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>30.282169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>51.189287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>28.523293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>43.573113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>40.644463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>13.67676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>28.523293</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    relevance_score qid sum_tf_idf\n",
              "0               0.0   4  30.667985\n",
              "1               0.0   4  42.200053\n",
              "2               0.0   4  30.667985\n",
              "3               0.0   4  30.282169\n",
              "4               1.0   4  51.189287\n",
              "..              ...  ..        ...\n",
              "98              0.0   4  28.523293\n",
              "99              1.0   4  43.573113\n",
              "100             2.0   4  40.644463\n",
              "101             1.0   4   13.67676\n",
              "102             0.0   4  28.523293\n",
              "\n",
              "[103 rows x 3 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = np.array(docs)\n",
        "df = pd.DataFrame(docs, columns=['relevance_score','qid', \"sum_tf_idf\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TJVuO-NnHOCX"
      },
      "outputs": [],
      "source": [
        "df['relevance_score'] = df['relevance_score'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q9XF4yljJZL9"
      },
      "outputs": [],
      "source": [
        "df_norm = df.sort_values('relevance_score', ascending=False)\n",
        "df_norm = df_norm.reset_index().drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YaF5WGJYJZOl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(     relevance_score qid sum_tf_idf  idx\n",
              " 0                0.0   4  30.667985    2\n",
              " 1                0.0   4  42.200053    3\n",
              " 2                0.0   4  30.667985    4\n",
              " 3                0.0   4  30.282169    5\n",
              " 4                1.0   4  51.189287    6\n",
              " ..               ...  ..        ...  ...\n",
              " 98               0.0   4  28.523293  100\n",
              " 99               1.0   4  43.573113  101\n",
              " 100              2.0   4  40.644463  102\n",
              " 101              1.0   4   13.67676  103\n",
              " 102              0.0   4  28.523293  104\n",
              " \n",
              " [103 rows x 4 columns],\n",
              "      relevance_score qid sum_tf_idf  idx\n",
              " 0                3.0   4  45.331988    2\n",
              " 1                2.0   4  17.978469    3\n",
              " 2                2.0   4   13.67676    4\n",
              " 3                2.0   4  22.665994    5\n",
              " 4                2.0   4  25.594644    6\n",
              " ..               ...  ..        ...  ...\n",
              " 98               0.0   4          0  100\n",
              " 99               0.0   4          0  101\n",
              " 100              0.0   4  51.189287  102\n",
              " 101              0.0   4  51.189287  103\n",
              " 102              0.0   4  28.523293  104\n",
              " \n",
              " [103 rows x 4 columns])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['idx'] = 0\n",
        "df_norm['idx'] = 0\n",
        "idx_val = 1\n",
        "for i in range(len(df)):\n",
        "    df.loc[i, \"idx\"] = idx_val+1\n",
        "    df_norm.loc[i, \"idx\"] = idx_val+1\n",
        "    idx_val+=1\n",
        "df, df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g11-UqKZJZRb"
      },
      "outputs": [],
      "source": [
        "df['log_i'] = np.log2(df['idx'])\n",
        "df_norm['log_i'] = np.log2(df_norm['idx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pMXWKSg8JZTz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>qid</th>\n",
              "      <th>sum_tf_idf</th>\n",
              "      <th>idx</th>\n",
              "      <th>log_i</th>\n",
              "      <th>iDCG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "      <td>45.331988</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>17.978469</td>\n",
              "      <td>3</td>\n",
              "      <td>1.584963</td>\n",
              "      <td>1.892789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>13.67676</td>\n",
              "      <td>4</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>22.665994</td>\n",
              "      <td>5</td>\n",
              "      <td>2.321928</td>\n",
              "      <td>1.292030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "      <td>25.594644</td>\n",
              "      <td>6</td>\n",
              "      <td>2.584963</td>\n",
              "      <td>1.160558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>6.643856</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>6.658211</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>51.189287</td>\n",
              "      <td>102</td>\n",
              "      <td>6.672425</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>51.189287</td>\n",
              "      <td>103</td>\n",
              "      <td>6.686501</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>28.523293</td>\n",
              "      <td>104</td>\n",
              "      <td>6.700440</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     relevance_score qid sum_tf_idf  idx     log_i      iDCG\n",
              "0                3.0   4  45.331988    2  1.000000  7.000000\n",
              "1                2.0   4  17.978469    3  1.584963  1.892789\n",
              "2                2.0   4   13.67676    4  2.000000  1.500000\n",
              "3                2.0   4  22.665994    5  2.321928  1.292030\n",
              "4                2.0   4  25.594644    6  2.584963  1.160558\n",
              "..               ...  ..        ...  ...       ...       ...\n",
              "98               0.0   4          0  100  6.643856  0.000000\n",
              "99               0.0   4          0  101  6.658211  0.000000\n",
              "100              0.0   4  51.189287  102  6.672425  0.000000\n",
              "101              0.0   4  51.189287  103  6.686501  0.000000\n",
              "102              0.0   4  28.523293  104  6.700440  0.000000\n",
              "\n",
              "[103 rows x 6 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"DCG\"] = (np.power(2, df[\"relevance_score\"]) - 1)/df[\"log_i\"]\n",
        "df_norm[\"iDCG\"] = (np.power(2, df_norm[\"relevance_score\"]) - 1)/df_norm[\"log_i\"]\n",
        "df_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3cwjB34uKk7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max DCG: 28.988467538734827\n"
          ]
        }
      ],
      "source": [
        "nDCG_normalize_term = df_norm[\"iDCG\"].sum()\n",
        "print(f\"Max DCG: {nDCG_normalize_term}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MoqzhRsDJZV5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of possible files:  19893497375938370599826047614905329896936840170566570588205180312704857992695193482412686565431050240000000000000000000000\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "num_possible_files = 1\n",
        "for reps in df[\"relevance_score\"].value_counts():\n",
        "    num_possible_files *= math.factorial(reps)\n",
        "print(\"Number of possible files: \", num_possible_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GnqhY7XrJZYr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nDCG at at 50:  0.35612494416255847\n",
            "nDCG at for the whole dataset:  0.5784691984582588\n"
          ]
        }
      ],
      "source": [
        "print(\"nDCG at at 50: \",df[\"DCG\"].iloc[:50].sum()/nDCG_normalize_term)\n",
        "print(\"nDCG at for the whole dataset: \",df[\"DCG\"].sum()/nDCG_normalize_term)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Jz9CXsYNJZa0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relevance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     relevance_score\n",
              "0                2.0\n",
              "1                0.0\n",
              "2                0.0\n",
              "3                1.0\n",
              "4                0.0\n",
              "..               ...\n",
              "98               0.0\n",
              "99               0.0\n",
              "100              0.0\n",
              "101              0.0\n",
              "102              0.0\n",
              "\n",
              "[103 rows x 1 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_docs = df.sort_values('sum_tf_idf', ascending=False)\n",
        "df_docs = df_docs[[\"relevance_score\"]]\n",
        "df_docs = df_docs.reset_index().drop([\"index\"], axis=1)\n",
        "df_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y54K65n2JZdd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relevance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>103 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     relevance_score\n",
              "0                  1\n",
              "1                  0\n",
              "2                  0\n",
              "3                  1\n",
              "4                  0\n",
              "..               ...\n",
              "98                 0\n",
              "99                 0\n",
              "100                0\n",
              "101                0\n",
              "102                0\n",
              "\n",
              "[103 rows x 1 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_docs['relevance_score'] = df_docs['relevance_score'].apply(lambda x : 1 if x > 0 else 0)\n",
        "df_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "precs = []\n",
        "recs = []\n",
        "for i in range(1,len(df_docs)):\n",
        "    preck = np.count_nonzero(df_docs[\"relevance_score\"].iloc[:i].to_numpy() == 1)/i\n",
        "    reck = np.count_nonzero(df_docs[\"relevance_score\"].iloc[:i].to_numpy() == 1)/np.count_nonzero(df_docs[\"relevance_score\"].to_numpy() == 1)\n",
        "    precs.append(preck)\n",
        "    recs.append(reck)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oO8YC9feJZgL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(recs, precs)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "148YNjPtH8TH"
      },
      "outputs": [],
      "source": [
        "!unzip /content/20_newsgroups.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQxUrQA7Abwj"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "dmntlr8RAbwk"
      },
      "outputs": [],
      "source": [
        "def preprocess(line):\n",
        "  '''\n",
        "  Preprocess the data\n",
        "    - Lowercase\n",
        "    - Remove Extra Spaces\n",
        "    - Lemmatize using WordNetLemmatizer()\n",
        "    - Tokenize using RegexpTokenizer()\n",
        "    - Remove stopwords\n",
        "  '''\n",
        "  line = \" \".join(line.lower().split()) #lowercase and remove extra spaces\n",
        "  #line = line.apply(lambda x: nltk.word_tokenize(x)) #tokenize\n",
        "  line = [word for word in line.split() if not word in stopwords] #remove stopwords\n",
        "  line = [lemmatizer.lemmatize(word) for word in line] #lemmatize\n",
        "  line = tokenizer.tokenize(\" \".join(line)) #tokenize\n",
        "  return nltk.FreqDist(line)\n",
        "\n",
        "data_dict = {}\n",
        "doc_word_set = {}\n",
        "class_mapping = {'comp.graphics': 0, 'sci.med': 1, 'talk.politics.misc': 2, 'rec.sport.hockey': 3, 'sci.space': 4}\n",
        "\n",
        "for clas in class_mapping.keys():\n",
        "  doc_word_set[clas] = {}\n",
        "  for n, filee in enumerate(glob.glob(f'20_newsgroups/{clas}/*')):\n",
        "    with open(filee, \"r\", encoding='windows-1254', errors='ignore') as fp:\n",
        "      fp = fp.read()\n",
        "      freq_dict = preprocess(fp)\n",
        "    for i in freq_dict:\n",
        "      if i not in data_dict:\n",
        "        data_dict[i] = [([n, filee.split(\"/\")[-1], class_mapping[clas]], freq_dict[i]/len(freq_dict))]\n",
        "      else:\n",
        "        data_dict[i].append(([n, filee.split(\"/\")[-1], class_mapping[clas]], freq_dict[i]/len(freq_dict)))\n",
        "    doc_word_set[clas][filee.split(\"/\")[-1]] = set(list(freq_dict.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RKnPDT_LAbwl"
      },
      "outputs": [],
      "source": [
        "#save the index\n",
        "with open('news_frequency_index.pkl', 'wb') as fp:\n",
        "    pickle.dump(data_dict, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "HY4KmtLLAbwl"
      },
      "outputs": [],
      "source": [
        "with open('news_frequency_index.pkl', 'rb') as fp:\n",
        "    data_dict = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "WGOYQ8UsAbwm"
      },
      "outputs": [],
      "source": [
        "vocab = list(data_dict.keys())\n",
        "tf_idf_mat = np.zeros(len(vocab))\n",
        "num_classes = 5\n",
        "\n",
        "for n, i in enumerate(list(data_dict.keys())):\n",
        "    num_different_classes = len(np.unique(np.array([x[0][-1] for x in data_dict[i]])))\n",
        "    num_occurences = sum([x[1] for x in data_dict[i]])\n",
        "    tf_idf_mat[n] = num_occurences * np.log(num_classes/num_different_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "rTTF7x5BE4WY"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(y_pred, y_test, binary=False, oseru_tonkachi=False):\n",
        "    \"\"\"\n",
        "    Custom implementation to calculate metrics for for a trained model\n",
        "    binary: True if data has binary labels, false otherwise\n",
        "    \"\"\"\n",
        "    if(oseru_tonkachi):\n",
        "        y_test =np.argmax(y_test, axis=1)\n",
        "        y_test = np.reshape(y_test,(-1,1))\n",
        "        y_pred =np.argmax(y_pred, axis=1)\n",
        "        y_pred = np.reshape(y_pred,(-1,1))\n",
        "    if(binary):\n",
        "        accuracy = (np.sum(y_pred==y_test))/y_pred.shape[0]\n",
        "        fn = np.sum((y_pred==0) & (y_test==1))\n",
        "        fp = np.sum((y_pred==1) & (y_test==0))\n",
        "        tn = np.sum((y_pred==0) & (y_test==0))\n",
        "        tp = np.sum((y_pred==1) & (y_test==1))\n",
        "        precision = tp/(tp+fp)\n",
        "        recall = tp/(tp+fn)\n",
        "        f1_score = 2*tp/((2*tp)+fp+fn)\n",
        "        print(tp, tn, fp, fn, \" < - Positives & Negs\")\n",
        "        print(precision, \" prec\")\n",
        "        print(recall, \" rec\")\n",
        "        print(accuracy, \" acc\")\n",
        "        print(f1_score, \" f1\")\n",
        "    else:\n",
        "        tpi = []\n",
        "        tni = []\n",
        "        fpi = []\n",
        "        fni = []\n",
        "        mac_prec = []\n",
        "        mac_rec = []\n",
        "        for i in np.unique(y_test):\n",
        "            fn = np.sum((y_pred!=i) & (y_test==i))\n",
        "            fp = np.sum((y_pred==i) & (y_test!=i))\n",
        "            tn = np.sum((y_pred!=i) & (y_test!=i))\n",
        "            tp = np.sum((y_pred==i) & (y_test==i))\n",
        "            tpi.append(tp)\n",
        "            tni.append(tn)\n",
        "            fpi.append(fp)\n",
        "            fni.append(fn)\n",
        "            mac_prec.append(tp/(tp+fp))\n",
        "            mac_rec.append(tp/(tp+fn))\n",
        "        micro_precision = sum(tpi)/(sum(tpi)+sum(fpi))\n",
        "        micro_recall = sum(tpi)/(sum(tpi)+sum(fni))\n",
        "        micro_f1 = 2*((micro_precision*micro_recall)/(micro_precision+micro_recall))\n",
        "        macro_precision = sum(mac_prec)/len(mac_prec)\n",
        "        macro_recall = sum(mac_rec)/len(mac_rec)\n",
        "        conf = confusion_matrix_2(y_pred, y_test)\n",
        "        acc = sum([conf[i,i] for i in range(conf.shape[0])])/np.sum(conf)\n",
        "        print(f\"micro_precision {micro_precision}, micro_recall {micro_recall}, micro_f1 {micro_f1}, macro_precision {macro_precision}, macro_recall {macro_recall}, accuracy {acc}\")\n",
        "        print(conf)\n",
        "\n",
        "\n",
        "def confusion_matrix_2(y_pred, y_test):\n",
        "    \"\"\"\n",
        "    Prints confusion matrix\n",
        "    \"\"\"\n",
        "    num_classes = np.unique(y_test)\n",
        "    conf = np.zeros((len(num_classes), len(num_classes)))\n",
        "    for actual in num_classes:\n",
        "        for pred in num_classes:\n",
        "            conf[pred, actual] = np.sum((y_pred==pred) & (y_test==actual))\n",
        "    return conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ZLLNCQ0_Abwm"
      },
      "outputs": [],
      "source": [
        "k = 150\n",
        "\n",
        "class_vocab_list = {'comp.graphics': [], 'sci.med': [], 'talk.politics.misc': [], 'rec.sport.hockey': [], 'sci.space': []}\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for clas in doc_word_set:\n",
        "    class_word_set = set([word for doc in doc_word_set[clas] for word in doc_word_set[clas][doc]])\n",
        "    tf_icf_pair = [(tf_idf_mat[vocab.index(word)], word) for word in class_word_set]\n",
        "    heapq.heapify(tf_icf_pair)\n",
        "    class_vocab_list[clas] = [pair[1] for pair in heapq.nlargest(k, tf_icf_pair)]\n",
        "\n",
        "effective_vocab = list(set(class_vocab_list['comp.graphics'] + class_vocab_list['sci.med'] + class_vocab_list['talk.politics.misc'] + class_vocab_list['rec.sport.hockey'] + class_vocab_list['sci.space']))\n",
        "for clas in doc_word_set:\n",
        "    for doc in doc_word_set[clas]:\n",
        "        feature_list = np.zeros((len(effective_vocab)))\n",
        "        for word in set(doc_word_set[clas][doc]).intersection(set(class_vocab_list[clas])):\n",
        "            feature_list[effective_vocab.index(word)] = tf_idf_mat[effective_vocab.index(word)]\n",
        "        x.append(feature_list)\n",
        "        y.append(class_mapping[clas])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "7vBBTQBkAbwn"
      },
      "outputs": [],
      "source": [
        "x = np.array(x)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlx2KyPeH31A",
        "outputId": "e1d18016-2cae-48d7-a810-f9a2d586744b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.04411412],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.04411412],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.04411412],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.04411412]])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Taz4eZLjGC8_",
        "outputId": "bf9000df-e8d1-4ffd-f2f4-417504d59af0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 479)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NdyJTM_8Bp5"
      },
      "source": [
        "## 80-20 Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "7spIZ77pAbwp"
      },
      "outputs": [],
      "source": [
        "filenames = np.array(glob.glob('20_newsgroups/comp.graphics/*') + glob.glob('20_newsgroups/sci.med/*') + glob.glob('20_newsgroups/talk.politics.misc/*') + glob.glob('20_newsgroups/rec.sport.hockey/*') + glob.glob('20_newsgroups/sci.space/*'))\n",
        "train_size = 0.8\n",
        "\n",
        "train = np.random.choice(range(filenames.shape[0]), int(train_size*filenames.shape[0]), replace=False)\n",
        "test = list(set(range(filenames.shape[0])) - set(train))\n",
        "\n",
        "X_train = x[train]\n",
        "y_train = y[train]\n",
        "X_test = x[test]\n",
        "y_test = y[test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaYYtMFZAbws",
        "outputId": "c5d4ae5c-5959-4015-fef3-d222c7794735"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "micro_precision 0.884, micro_recall 0.884, micro_f1 0.884, macro_precision 0.8977476616454594, macro_recall 0.8829267832622344, accuracy 0.884\n",
            "[[170.   1.  39.   0.   2.]\n",
            " [  2. 133.   0.   1.   1.]\n",
            " [  4.   7. 166.   0.   0.]\n",
            " [  0.   0.   0. 208.   0.]\n",
            " [  3.  54.   2.   0. 207.]]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "prior_probs = {} # Prior probablities of classe\n",
        "num_classes = 0     # Gets updated to number of classes\n",
        "mean_std = {}       # Gets updated after training, stores mean and std dev of each feature classwise\n",
        "best_class_guess = 0 # Class with highes prior probability, used during prediction\n",
        "\n",
        "def prior(y):\n",
        "    \"\"\"\n",
        "    Calculates Prior probabilites for classes\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    for i in np.unique(y):\n",
        "        probs[i] = np.sum(y==i)/len(y)\n",
        "    return probs\n",
        "\n",
        "def seperate_samples(X, y):\n",
        "    \"\"\"\n",
        "    Separates the samples into classes\n",
        "    \"\"\"\n",
        "    classes = np.unique(y)\n",
        "    temp = {}\n",
        "    for i in classes:\n",
        "        this_class = X[np.where(y==i)[0]] # Get sample which belongs to class i\n",
        "        temp[i] = this_class\n",
        "    return temp\n",
        "\n",
        "def calc_mean_std(X):\n",
        "    \"\"\"\n",
        "    Calculate Mean and Stdev for each feature\n",
        "    \"\"\"\n",
        "    values = []\n",
        "    means = np.mean(X, axis=0)\n",
        "    stds = np.std(X, axis=0)\n",
        "    values.append(means)\n",
        "    values.append(stds)\n",
        "    return values\n",
        "\n",
        "def classwise_mean_std(X, y):\n",
        "    \"\"\"\n",
        "    Calculate mean, stdev of features class wise\n",
        "    returns a classwise dictionary\n",
        "    \"\"\"\n",
        "    separated_classes = seperate_samples(X,y)\n",
        "    mean_stds = []\n",
        "    class_dict = {}\n",
        "    for i in separated_classes:\n",
        "        temp = calc_mean_std(separated_classes[i])\n",
        "        class_dict[i] = temp\n",
        "    return class_dict\n",
        "\n",
        "def fit(X, y):\n",
        "    \"\"\"\n",
        "    Wrapper to calculate classwise mean and stdev\n",
        "    \"\"\"\n",
        "    to_ret = classwise_mean_std(X, y)\n",
        "    return to_ret\n",
        "\n",
        "def gaussian(x, mu, sigma):\n",
        "    \"\"\"\n",
        "    Calculates the Gaussian Likelihood.\n",
        "    \"\"\"\n",
        "    exponent = exp(-((x-mu)**2/(2*(sigma+1e-6)**2))) # Added 1e-6 for the case when sigma is 0\n",
        "    return (1/(sqrt(2*pi)*(sigma+1e-6)))*exponent\n",
        "\n",
        "def pred_helper(X):\n",
        "    \"\"\"\n",
        "    Predicts class for each 1 x num_features sample array.\n",
        "    Returns best class guess\n",
        "    \"\"\"\n",
        "    class_guess = best_class_guess # Set to the class with max apriori probability\n",
        "    best_sum = -math.inf\n",
        "    for i in range(num_classes):\n",
        "        temp_sum = np.log(prior_probs[i])\n",
        "        for j in range(len(X)):\n",
        "            adder = np.log(gaussian(X[j], mean_std[i][0][j], mean_std[i][1][j]))\n",
        "            if(adder!=None):\n",
        "                temp_sum+=adder\n",
        "        if(temp_sum>best_sum):\n",
        "            best_sum = temp_sum\n",
        "            class_guess = i\n",
        "    return class_guess\n",
        "\n",
        "def predict(X):\n",
        "    \"\"\"\n",
        "    Wrapper for pred_helper function.\n",
        "    Returns predictions\n",
        "    \"\"\" \n",
        "    y_pred = []\n",
        "    for i in X:\n",
        "        lolz = pred_helper(i)\n",
        "        y_pred.append(lolz)\n",
        "    return y_pred\n",
        "\n",
        "prior_probs = prior(y)\n",
        "num_classes = len(np.unique(y))\n",
        "for i in prior_probs:\n",
        "    if(prior_probs[i]>best_class_guess):\n",
        "        best_class_guess = i\n",
        "mean_std = fit(X_train,y_train)\n",
        "y_pred = predict(X_test)     \n",
        "print(eval_metrics(y_pred, y_test))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUa4I_o8Bp7"
      },
      "source": [
        "## 70-30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLbndTfh8Bp8",
        "outputId": "802452a2-23a3-4233-9c3c-a2003be328be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "micro_precision 0.828, micro_recall 0.828, micro_f1 0.828, macro_precision 0.8673011395565181, macro_recall 0.8308409607375526, accuracy 0.828\n",
            "[[189.   1.   0.   0.   4.]\n",
            " [  1. 191.   3.   2.   4.]\n",
            " [112.  24. 287.   0.   4.]\n",
            " [  0.   0.   0. 282.   0.]\n",
            " [  5.  88.  10.   0. 293.]]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "filenames = np.array(glob.glob('20_newsgroups/comp.graphics/*') + glob.glob('20_newsgroups/sci.med/*') + glob.glob('20_newsgroups/talk.politics.misc/*') + glob.glob('20_newsgroups/rec.sport.hockey/*') + glob.glob('20_newsgroups/sci.space/*'))\n",
        "train_size = 0.7\n",
        "\n",
        "train = np.random.choice(range(filenames.shape[0]), int(train_size*filenames.shape[0]), replace=False)\n",
        "test = list(set(range(filenames.shape[0])) - set(train))\n",
        "\n",
        "X_train = x[train]\n",
        "y_train = y[train]\n",
        "X_test = x[test]\n",
        "y_test = y[test]\n",
        "\n",
        "prior_probs = {} # Prior probablities of classe\n",
        "num_classes = 0     # Gets updated to number of classes\n",
        "mean_std = {}       # Gets updated after training, stores mean and std dev of each feature classwise\n",
        "best_class_guess = 0 # Class with highes prior probability, used during prediction\n",
        "\n",
        "def prior(y):\n",
        "    \"\"\"\n",
        "    Calculates Prior probabilites for classes\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    for i in np.unique(y):\n",
        "        probs[i] = np.sum(y==i)/len(y)\n",
        "    return probs\n",
        "\n",
        "def seperate_samples(X, y):\n",
        "    \"\"\"\n",
        "    Separates the samples into classes\n",
        "    \"\"\"\n",
        "    classes = np.unique(y)\n",
        "    temp = {}\n",
        "    for i in classes:\n",
        "        this_class = X[np.where(y==i)[0]] # Get sample which belongs to class i\n",
        "        temp[i] = this_class\n",
        "    return temp\n",
        "\n",
        "def calc_mean_std(X):\n",
        "    \"\"\"\n",
        "    Calculate Mean and Stdev for each feature\n",
        "    \"\"\"\n",
        "    values = []\n",
        "    means = np.mean(X, axis=0)\n",
        "    stds = np.std(X, axis=0)\n",
        "    values.append(means)\n",
        "    values.append(stds)\n",
        "    return values\n",
        "\n",
        "def classwise_mean_std(X, y):\n",
        "    \"\"\"\n",
        "    Calculate mean, stdev of features class wise\n",
        "    returns a classwise dictionary\n",
        "    \"\"\"\n",
        "    separated_classes = seperate_samples(X,y)\n",
        "    mean_stds = []\n",
        "    class_dict = {}\n",
        "    for i in separated_classes:\n",
        "        temp = calc_mean_std(separated_classes[i])\n",
        "        class_dict[i] = temp\n",
        "    return class_dict\n",
        "\n",
        "def fit(X, y):\n",
        "    \"\"\"\n",
        "    Wrapper to calculate classwise mean and stdev\n",
        "    \"\"\"\n",
        "    to_ret = classwise_mean_std(X, y)\n",
        "    return to_ret\n",
        "\n",
        "def gaussian(x, mu, sigma):\n",
        "    \"\"\"\n",
        "    Calculates the Gaussian Likelihood.\n",
        "    \"\"\"\n",
        "    exponent = exp(-((x-mu)**2/(2*(sigma+1e-6)**2))) # Added 1e-6 for the case when sigma is 0\n",
        "    return (1/(sqrt(2*pi)*(sigma+1e-6)))*exponent\n",
        "\n",
        "def pred_helper(X):\n",
        "    \"\"\"\n",
        "    Predicts class for each 1 x num_features sample array.\n",
        "    Returns best class guess\n",
        "    \"\"\"\n",
        "    class_guess = best_class_guess # Set to the class with max apriori probability\n",
        "    best_sum = -math.inf\n",
        "    for i in range(num_classes):\n",
        "        temp_sum = np.log(prior_probs[i])\n",
        "        for j in range(len(X)):\n",
        "            adder = np.log(gaussian(X[j], mean_std[i][0][j], mean_std[i][1][j]))\n",
        "            if(adder!=None):\n",
        "                temp_sum+=adder\n",
        "        if(temp_sum>best_sum):\n",
        "            best_sum = temp_sum\n",
        "            class_guess = i\n",
        "    return class_guess\n",
        "\n",
        "def predict(X):\n",
        "    \"\"\"\n",
        "    Wrapper for pred_helper function.\n",
        "    Returns predictions\n",
        "    \"\"\" \n",
        "    y_pred = []\n",
        "    for i in X:\n",
        "        lolz = pred_helper(i)\n",
        "        y_pred.append(lolz)\n",
        "    return y_pred\n",
        "\n",
        "prior_probs = prior(y)\n",
        "num_classes = len(np.unique(y))\n",
        "for i in prior_probs:\n",
        "    if(prior_probs[i]>best_class_guess):\n",
        "        best_class_guess = i\n",
        "mean_std = fit(X_train,y_train)\n",
        "y_pred = predict(X_test)        \n",
        "print(eval_metrics(y_pred, y_test))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Icxqun8Bp9"
      },
      "source": [
        "## 50-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D41Vhi9U8Bp9",
        "outputId": "1c83cffc-8d29-4996-d82a-5b150413ee1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "micro_precision 0.8592, micro_recall 0.8592, micro_f1 0.8592000000000001, macro_precision 0.8756760823617826, macro_recall 0.860178778556391, accuracy 0.8592\n",
            "[[467.   3. 103.   0.   1.]\n",
            " [  7. 329.   4.   4.   5.]\n",
            " [  7.  33. 369.   0.   4.]\n",
            " [  0.   0.   0. 497.   0.]\n",
            " [ 21. 152.   8.   0. 486.]]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "filenames = np.array(glob.glob('20_newsgroups/comp.graphics/*') + glob.glob('20_newsgroups/sci.med/*') + glob.glob('20_newsgroups/talk.politics.misc/*') + glob.glob('20_newsgroups/rec.sport.hockey/*') + glob.glob('20_newsgroups/sci.space/*'))\n",
        "train_size = 0.5\n",
        "\n",
        "train = np.random.choice(range(filenames.shape[0]), int(train_size*filenames.shape[0]), replace=False)\n",
        "test = list(set(range(filenames.shape[0])) - set(train))\n",
        "\n",
        "X_train = x[train]\n",
        "y_train = y[train]\n",
        "X_test = x[test]\n",
        "y_test = y[test]\n",
        "\n",
        "prior_probs = {} # Prior probablities of classe\n",
        "num_classes = 0     # Gets updated to number of classes\n",
        "mean_std = {}       # Gets updated after training, stores mean and std dev of each feature classwise\n",
        "best_class_guess = 0 # Class with highes prior probability, used during prediction\n",
        "\n",
        "def prior(y):\n",
        "    \"\"\"\n",
        "    Calculates Prior probabilites for classes\n",
        "    \"\"\"\n",
        "    probs = {}\n",
        "    for i in np.unique(y):\n",
        "        probs[i] = np.sum(y==i)/len(y)\n",
        "    return probs\n",
        "\n",
        "def seperate_samples(X, y):\n",
        "    \"\"\"\n",
        "    Separates the samples into classes\n",
        "    \"\"\"\n",
        "    classes = np.unique(y)\n",
        "    temp = {}\n",
        "    for i in classes:\n",
        "        this_class = X[np.where(y==i)[0]] # Get sample which belongs to class i\n",
        "        temp[i] = this_class\n",
        "    return temp\n",
        "\n",
        "def calc_mean_std(X):\n",
        "    \"\"\"\n",
        "    Calculate Mean and Stdev for each feature\n",
        "    \"\"\"\n",
        "    values = []\n",
        "    means = np.mean(X, axis=0)\n",
        "    stds = np.std(X, axis=0)\n",
        "    values.append(means)\n",
        "    values.append(stds)\n",
        "    return values\n",
        "\n",
        "def classwise_mean_std(X, y):\n",
        "    \"\"\"\n",
        "    Calculate mean, stdev of features class wise\n",
        "    returns a classwise dictionary\n",
        "    \"\"\"\n",
        "    separated_classes = seperate_samples(X,y)\n",
        "    mean_stds = []\n",
        "    class_dict = {}\n",
        "    for i in separated_classes:\n",
        "        temp = calc_mean_std(separated_classes[i])\n",
        "        class_dict[i] = temp\n",
        "    return class_dict\n",
        "\n",
        "def fit(X, y):\n",
        "    \"\"\"\n",
        "    Wrapper to calculate classwise mean and stdev\n",
        "    \"\"\"\n",
        "    to_ret = classwise_mean_std(X, y)\n",
        "    return to_ret\n",
        "\n",
        "def gaussian(x, mu, sigma):\n",
        "    \"\"\"\n",
        "    Calculates the Gaussian Likelihood.\n",
        "    \"\"\"\n",
        "    exponent = exp(-((x-mu)**2/(2*(sigma+1e-6)**2))) # Added 1e-6 for the case when sigma is 0\n",
        "    return (1/(sqrt(2*pi)*(sigma+1e-6)))*exponent\n",
        "\n",
        "def pred_helper(X):\n",
        "    \"\"\"\n",
        "    Predicts class for each 1 x num_features sample array.\n",
        "    Returns best class guess\n",
        "    \"\"\"\n",
        "    class_guess = best_class_guess # Set to the class with max apriori probability\n",
        "    best_sum = -math.inf\n",
        "    for i in range(num_classes):\n",
        "        temp_sum = np.log(prior_probs[i])\n",
        "        for j in range(len(X)):\n",
        "            adder = np.log(gaussian(X[j], mean_std[i][0][j], mean_std[i][1][j]))\n",
        "            if(adder!=None):\n",
        "                temp_sum+=adder\n",
        "        if(temp_sum>best_sum):\n",
        "            best_sum = temp_sum\n",
        "            class_guess = i\n",
        "    return class_guess\n",
        "\n",
        "def predict(X):\n",
        "    \"\"\"\n",
        "    Wrapper for pred_helper function.\n",
        "    Returns predictions\n",
        "    \"\"\" \n",
        "    y_pred = []\n",
        "    for i in X:\n",
        "        lolz = pred_helper(i)\n",
        "        y_pred.append(lolz)\n",
        "    return y_pred\n",
        "\n",
        "prior_probs = prior(y)\n",
        "num_classes = len(np.unique(y))\n",
        "for i in prior_probs:\n",
        "    if(prior_probs[i]>best_class_guess):\n",
        "        best_class_guess = i\n",
        "mean_std = fit(X_train,y_train)\n",
        "y_pred = predict(X_test)        \n",
        "print(eval_metrics(y_pred, y_test))   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "JAZUi8G2PKHf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR_A2_Final.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5259a5ddbbbf560fa7fca35d254d2293fd3cfc618e59deb3b9bb88fde2e4da52"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('myenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
